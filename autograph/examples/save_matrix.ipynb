{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import h5py\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "# import zlib\n",
    "# from typing import Union, Dict, Any, List\n",
    "# import scipy.sparse as sparse\n",
    "\n",
    "# class MatrixStorage:\n",
    "#     \"\"\"Utility class for efficient matrix storage with different formats.\"\"\"\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def save_matrices(matrices: Dict[str, Union[np.ndarray, sparse.spmatrix]], \n",
    "#                      filepath: str,\n",
    "#                      format: str = 'npz',\n",
    "#                      compress: bool = True,\n",
    "#                      metadata: Dict[str, Any] = None) -> None:\n",
    "#         \"\"\"\n",
    "#         Save multiple matrices in a single file with their associated keys.\n",
    "        \n",
    "#         Args:\n",
    "#             matrices: Dictionary mapping keys to matrices\n",
    "#             filepath: Path to save the file\n",
    "#             format: Format to save ('npz', 'hdf5')\n",
    "#             compress: Whether to use compression\n",
    "#             metadata: Optional dictionary of metadata to save\n",
    "#         \"\"\"\n",
    "#         filepath = Path(filepath)\n",
    "#         filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         if format == 'npz':\n",
    "#             save_dict = {}\n",
    "#             sparse_info = {}\n",
    "            \n",
    "#             for key, matrix in matrices.items():\n",
    "#                 if sparse.issparse(matrix):\n",
    "#                     # For sparse matrices, save the format and convert to COO\n",
    "#                     matrix = matrix.tocoo()\n",
    "#                     save_dict[f\"{key}_data\"] = matrix.data\n",
    "#                     save_dict[f\"{key}_row\"] = matrix.row\n",
    "#                     save_dict[f\"{key}_col\"] = matrix.col\n",
    "#                     sparse_info[key] = {\n",
    "#                         'shape': matrix.shape,\n",
    "#                         'format': 'sparse'\n",
    "#                     }\n",
    "#                 else:\n",
    "#                     save_dict[key] = matrix\n",
    "            \n",
    "#             if metadata or sparse_info:\n",
    "#                 save_dict['__metadata__'] = {\n",
    "#                     'user_metadata': metadata if metadata else {},\n",
    "#                     'sparse_info': sparse_info\n",
    "#                 }\n",
    "            \n",
    "#             if compress:\n",
    "#                 np.savez_compressed(filepath, **save_dict)\n",
    "#             else:\n",
    "#                 np.savez(filepath, **save_dict)\n",
    "                \n",
    "#         elif format == 'hdf5':\n",
    "#             with h5py.File(filepath, 'w') as f:\n",
    "#                 for key, matrix in matrices.items():\n",
    "#                     if sparse.issparse(matrix):\n",
    "#                         if not isinstance(matrix, sparse.coo_matrix):\n",
    "#                             matrix = matrix.tocoo()\n",
    "                        \n",
    "#                         g = f.create_group(key)\n",
    "#                         g.create_dataset('data', data=matrix.data, \n",
    "#                                        compression='gzip' if compress else None)\n",
    "#                         g.create_dataset('row', data=matrix.row, \n",
    "#                                        compression='gzip' if compress else None)\n",
    "#                         g.create_dataset('col', data=matrix.col, \n",
    "#                                        compression='gzip' if compress else None)\n",
    "#                         g.attrs['shape'] = matrix.shape\n",
    "#                         g.attrs['format'] = 'sparse'\n",
    "#                     else:\n",
    "#                         f.create_dataset(key, data=matrix, \n",
    "#                                        compression='gzip' if compress else None)\n",
    "                \n",
    "#                 if metadata:\n",
    "#                     # Properly serialize metadata to JSON string\n",
    "#                     f.create_dataset('__metadata__', data=json.dumps(metadata))\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unsupported format: {format}\")\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def load_matrices(filepath: str) -> tuple[Dict[str, Union[np.ndarray, sparse.spmatrix]], Dict[str, Any]]:\n",
    "#         \"\"\"\n",
    "#         Load multiple matrices from file.\n",
    "        \n",
    "#         Returns:\n",
    "#             Tuple of (dictionary of matrices, metadata)\n",
    "#         \"\"\"\n",
    "#         filepath = Path(filepath)\n",
    "        \n",
    "#         if filepath.suffix == '.npz':\n",
    "#             with np.load(filepath, allow_pickle=True) as data:\n",
    "#                 matrices = {}\n",
    "#                 metadata = {}\n",
    "#                 sparse_info = {}\n",
    "                \n",
    "#                 # Load metadata if it exists\n",
    "#                 if '__metadata__' in data:\n",
    "#                     meta_dict = data['__metadata__'].item()\n",
    "#                     metadata = meta_dict.get('user_metadata', {})\n",
    "#                     sparse_info = meta_dict.get('sparse_info', {})\n",
    "                \n",
    "#                 # Process each key\n",
    "#                 for key in data.files:\n",
    "#                     if key == '__metadata__':\n",
    "#                         continue\n",
    "                        \n",
    "#                     # Check if this is part of a sparse matrix\n",
    "#                     base_key = key.split('_')[0]\n",
    "#                     if base_key in sparse_info:\n",
    "#                         if key.endswith('_data'):\n",
    "#                             # Reconstruct sparse matrix\n",
    "#                             data_arr = data[f\"{base_key}_data\"]\n",
    "#                             row_arr = data[f\"{base_key}_row\"]\n",
    "#                             col_arr = data[f\"{base_key}_col\"]\n",
    "#                             shape = sparse_info[base_key]['shape']\n",
    "                            \n",
    "#                             matrices[base_key] = sparse.coo_matrix(\n",
    "#                                 (data_arr, (row_arr, col_arr)),\n",
    "#                                 shape=shape\n",
    "#                             )\n",
    "#                     elif not key.endswith(('_row', '_col')):  # Skip sparse matrix components\n",
    "#                         matrices[key] = data[key]\n",
    "                \n",
    "#                 return matrices, metadata\n",
    "                    \n",
    "#         elif filepath.suffix == '.h5':\n",
    "#             with h5py.File(filepath, 'r') as f:\n",
    "#                 matrices = {}\n",
    "#                 metadata = {}\n",
    "                \n",
    "#                 # Load metadata if it exists\n",
    "#                 if '__metadata__' in f:\n",
    "#                     try:\n",
    "#                         metadata = json.loads(f['__metadata__'][()])\n",
    "#                     except json.JSONDecodeError:\n",
    "#                         # Handle case where metadata might be stored as string\n",
    "#                         metadata = {}\n",
    "                \n",
    "#                 # Load each matrix\n",
    "#                 for key in f.keys():\n",
    "#                     if key == '__metadata__':\n",
    "#                         continue\n",
    "                        \n",
    "#                     if isinstance(f[key], h5py.Group):\n",
    "#                         # Load sparse matrix\n",
    "#                         g = f[key]\n",
    "#                         if 'format' in g.attrs and g.attrs['format'] == 'sparse':\n",
    "#                             matrices[key] = sparse.coo_matrix(\n",
    "#                                 (g['data'][:], (g['row'][:], g['col'][:])),\n",
    "#                                 shape=g.attrs['shape']\n",
    "#                             )\n",
    "#                     else:\n",
    "#                         matrices[key] = f[key][:]\n",
    "                \n",
    "#                 return matrices, metadata\n",
    "        \n",
    "#         raise ValueError(f\"Unsupported file format: {filepath.suffix}\")\n",
    "\n",
    "#     @staticmethod\n",
    "#     def get_keys(filepath: str) -> List[str]:\n",
    "#         \"\"\"\n",
    "#         Get the keys of stored matrices without loading the data.\n",
    "        \n",
    "#         Args:\n",
    "#             filepath: Path to the file\n",
    "            \n",
    "#         Returns:\n",
    "#             List of matrix keys\n",
    "#         \"\"\"\n",
    "#         filepath = Path(filepath)\n",
    "        \n",
    "#         if filepath.suffix == '.npz':\n",
    "#             with np.load(filepath, allow_pickle=True) as data:\n",
    "#                 keys = set()\n",
    "#                 sparse_info = {}\n",
    "                \n",
    "#                 # Load sparse info if it exists\n",
    "#                 if '__metadata__' in data:\n",
    "#                     meta_dict = data['__metadata__'].item()\n",
    "#                     sparse_info = meta_dict.get('sparse_info', {})\n",
    "                \n",
    "#                 # Get unique keys, handling sparse matrix components\n",
    "#                 for key in data.files:\n",
    "#                     if key == '__metadata__':\n",
    "#                         continue\n",
    "#                     base_key = key.split('_')[0]\n",
    "#                     if base_key in sparse_info:\n",
    "#                         keys.add(base_key)\n",
    "#                     elif not key.endswith(('_row', '_col', '_data')):\n",
    "#                         keys.add(key)\n",
    "                \n",
    "#                 return sorted(list(keys))\n",
    "                \n",
    "#         elif filepath.suffix == '.h5':\n",
    "#             with h5py.File(filepath, 'r') as f:\n",
    "#                 return [k for k in f.keys() if k != '__metadata__']\n",
    "        \n",
    "#         raise ValueError(f\"Unsupported file format: {filepath.suffix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create sample matrices\n",
    "# matrices = {\n",
    "#     'dense_matrix': [np.ones((500, 300)), np.random.rand(1000, 1000)],\n",
    "#     'sparse_matrix': sparse.random(1000, 1000, density=0.01),\n",
    "#     'small_matrix': np.random.rand(10, 10)\n",
    "# }\n",
    "\n",
    "# # Save all matrices with metadata\n",
    "# MatrixStorage.save_matrices(\n",
    "#     matrices,\n",
    "#     'multiple_matrices.h5',\n",
    "#     format='hdf5',\n",
    "#     compress=True,\n",
    "#     metadata={'description': 'Collection of test matrices'}\n",
    "# )\n",
    "\n",
    "# # Get keys without loading data\n",
    "# keys = MatrixStorage.get_keys('multiple_matrices.h5')\n",
    "# print(f\"Available matrices: {keys}\")\n",
    "\n",
    "# # Load all matrices\n",
    "# loaded_matrices, metadata = MatrixStorage.load_matrices('multiple_matrices.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Layer', 'name', 'W')\n"
     ]
    }
   ],
   "source": [
    "layer_name, param = \"Layer.name.W\".rsplit('.', 1)\n",
    "layer, name = layer_name.split('.', 1)\n",
    "\n",
    "print((layer, name, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_matrices[\"dense_matrix\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Union, Dict, Any, List\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "class TensorStorage:\n",
    "    @staticmethod\n",
    "    def save_tensors(tensors: Dict[str, Union[np.ndarray, sparse.spmatrix, List[np.ndarray], List[sparse.spmatrix]]], \n",
    "                     filepath: str,\n",
    "                     format: str = 'hdf5',\n",
    "                     compress: bool = True,\n",
    "                     metadata: Dict[str, Any] = None) -> None:\n",
    "        filepath = Path(filepath)\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if format == 'hdf5':\n",
    "            with h5py.File(filepath, 'w') as f:\n",
    "                for key, tensor in tensors.items():\n",
    "                    if isinstance(tensor, list):\n",
    "                        g = f.create_group(key)\n",
    "                        g.attrs['is_list'] = True\n",
    "                        for i, t in enumerate(tensor):\n",
    "                            if sparse.issparse(t):\n",
    "                                sg = g.create_group(f'item_{i}')\n",
    "                                t = t.tocoo()\n",
    "                                sg.create_dataset('data', data=t.data, compression='gzip' if compress else None)\n",
    "                                sg.create_dataset('row', data=t.row, compression='gzip' if compress else None)\n",
    "                                sg.create_dataset('col', data=t.col, compression='gzip' if compress else None)\n",
    "                                sg.attrs['shape'] = t.shape\n",
    "                                sg.attrs['format'] = 'sparse'\n",
    "                            else:\n",
    "                                g.create_dataset(f'item_{i}', data=t, compression='gzip' if compress else None)\n",
    "                    else:\n",
    "                        if sparse.issparse(tensor):\n",
    "                            g = f.create_group(key)\n",
    "                            tensor = tensor.tocoo()\n",
    "                            g.create_dataset('data', data=tensor.data, compression='gzip' if compress else None)\n",
    "                            g.create_dataset('row', data=tensor.row, compression='gzip' if compress else None)\n",
    "                            g.create_dataset('col', data=tensor.col, compression='gzip' if compress else None)\n",
    "                            g.attrs['shape'] = tensor.shape\n",
    "                            g.attrs['format'] = 'sparse'\n",
    "                        else:\n",
    "                            f.create_dataset(key, data=tensor, compression='gzip' if compress else None)\n",
    "                \n",
    "                if metadata:\n",
    "                    f.create_dataset('__metadata__', data=json.dumps(metadata))\n",
    "        else:\n",
    "            raise ValueError(\"Only HDF5 format supported for tensor lists\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_tensors(filepath: str) -> tuple[Dict[str, Union[np.ndarray, sparse.spmatrix, List[Union[np.ndarray, sparse.spmatrix]]]], Dict[str, Any]]:\n",
    "        filepath = Path(filepath)\n",
    "        \n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            tensors = {}\n",
    "            metadata = {}\n",
    "            \n",
    "            if '__metadata__' in f:\n",
    "                metadata = json.loads(f['__metadata__'][()])\n",
    "            \n",
    "            for key in f.keys():\n",
    "                if key == '__metadata__':\n",
    "                    continue\n",
    "                    \n",
    "                if isinstance(f[key], h5py.Group):\n",
    "                    if 'is_list' in f[key].attrs:\n",
    "                        tensor_list = []\n",
    "                        for i in range(len(f[key].keys())):\n",
    "                            item = f[key][f'item_{i}']\n",
    "                            if isinstance(item, h5py.Group):\n",
    "                                tensor_list.append(sparse.coo_matrix(\n",
    "                                    (item['data'][:], (item['row'][:], item['col'][:])),\n",
    "                                    shape=item.attrs['shape']\n",
    "                                ))\n",
    "                            else:\n",
    "                                tensor_list.append(item[:])\n",
    "                        tensors[key] = tensor_list\n",
    "                    else:\n",
    "                        g = f[key]\n",
    "                        tensors[key] = sparse.coo_matrix(\n",
    "                            (g['data'][:], (g['row'][:], g['col'][:])),\n",
    "                            shape=g.attrs['shape']\n",
    "                        )\n",
    "                else:\n",
    "                    tensors[key] = f[key][:]\n",
    "            \n",
    "            return tensors, metadata\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = {\n",
    "    'single_tensor': np.random.rand(10, 10),\n",
    "    'tensor_list': [np.random.rand(5, 5) for _ in range(3)],\n",
    "    'mixed_list': [np.random.rand(3, 3), sparse.random(10, 10, density=0.1)]\n",
    "}\n",
    "\n",
    "storage = TensorStorage()\n",
    "storage.save_tensors(tensors, './models/tensors.h5')\n",
    "loaded_tensors, _ = storage.load_tensors('tensors.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './fold/test.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fold/test.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      2\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m}, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\oussa\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './fold/test.json'"
     ]
    }
   ],
   "source": [
    "with open(\"./fold/test.json\", \"w\") as f:\n",
    "    json.dump({\"a\":\"b\"}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mixed_list': [array([[0.06383093, 0.74544926, 0.48400633],\n",
       "         [0.36323882, 0.9379993 , 0.41887101],\n",
       "         [0.13188227, 0.30579926, 0.47064655]]),\n",
       "  <10x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 10 stored elements in COOrdinate format>],\n",
       " 'single_tensor': array([[0.9837431 , 0.39758005, 0.80212789, 0.21266189, 0.03932   ,\n",
       "         0.65569318, 0.80397452, 0.04333962, 0.56686855, 0.71105882],\n",
       "        [0.2478063 , 0.4807597 , 0.1936631 , 0.97125188, 0.8909481 ,\n",
       "         0.63782771, 0.2273844 , 0.27743369, 0.71810272, 0.7007975 ],\n",
       "        [0.21204309, 0.34950067, 0.29982319, 0.63328279, 0.9988186 ,\n",
       "         0.62824733, 0.77554709, 0.27468339, 0.14878811, 0.17153857],\n",
       "        [0.33975458, 0.40186554, 0.1133076 , 0.22064442, 0.41271228,\n",
       "         0.91106548, 0.88642493, 0.43685455, 0.74526915, 0.9271316 ],\n",
       "        [0.46212798, 0.79869936, 0.92611877, 0.14229006, 0.91762228,\n",
       "         0.09397049, 0.71861177, 0.18176957, 0.25439358, 0.35716383],\n",
       "        [0.07607653, 0.14542571, 0.47939653, 0.63828344, 0.71883814,\n",
       "         0.88440776, 0.42054114, 0.52410023, 0.05004685, 0.29559279],\n",
       "        [0.10221511, 0.07301367, 0.27382401, 0.08659133, 0.53780333,\n",
       "         0.7141507 , 0.59500166, 0.34120849, 0.36409963, 0.74348732],\n",
       "        [0.1677361 , 0.96693545, 0.32876827, 0.96616017, 0.78968679,\n",
       "         0.56343303, 0.44977924, 0.3154885 , 0.06663988, 0.88513306],\n",
       "        [0.2433199 , 0.16669774, 0.21823146, 0.91711255, 0.2647824 ,\n",
       "         0.83332499, 0.59312446, 0.17409535, 0.23350002, 0.36628343],\n",
       "        [0.71888245, 0.77078528, 0.09138488, 0.9778753 , 0.55254116,\n",
       "         0.20877457, 0.01775699, 0.46554431, 0.39271542, 0.15140615]]),\n",
       " 'tensor_list': [array([[0.96291131, 0.10266452, 0.07472264, 0.5521244 , 0.62286703],\n",
       "         [0.61438445, 0.45687184, 0.62687719, 0.46006092, 0.27780103],\n",
       "         [0.03170106, 0.88683611, 0.3198999 , 0.92092118, 0.13216012],\n",
       "         [0.507251  , 0.21879343, 0.94675399, 0.91522177, 0.89757585],\n",
       "         [0.47418885, 0.17816354, 0.42721505, 0.5201446 , 0.50562633]]),\n",
       "  array([[0.01310341, 0.88813744, 0.38595362, 0.01932491, 0.74195787],\n",
       "         [0.20976228, 0.51462871, 0.73352958, 0.6842174 , 0.72728847],\n",
       "         [0.51114869, 0.47302538, 0.14905253, 0.98868344, 0.7880314 ],\n",
       "         [0.50940046, 0.12504222, 0.26973768, 0.41753037, 0.68794994],\n",
       "         [0.76859237, 0.52392294, 0.36587948, 0.78615993, 0.60367516]]),\n",
       "  array([[0.70889498, 0.62677506, 0.42007789, 0.46681611, 0.53041958],\n",
       "         [0.40481348, 0.59653598, 0.57570543, 0.4439864 , 0.83591622],\n",
       "         [0.56011481, 0.2562351 , 0.79656923, 0.92029117, 0.97131332],\n",
       "         [0.24280648, 0.09553596, 0.68595763, 0.13571095, 0.4561793 ],\n",
       "         [0.36554426, 0.09409185, 0.43715877, 0.13549505, 0.52412743]])]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_tensors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
